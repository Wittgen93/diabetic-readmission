# Итоговый отчёт по проекту «Диабетические повторные госпитализации»

В этом репозитории вы найдёте единый ноутбук `7_Final_submission.ipynb`, который шаг за шагом показывает полный рабочий процесс: от загрузки сырых данных до финальных моделей и выводов.

---

## 1. Подготовка данных (`1_data_preparation.ipynb`)

**Что** мы сделали  
- Загрузили исходный CSV с информацией о госпитализациях диабетиков.  
- Заминировали все строки со значением `"?"` в `NaN`, посчитали долю пропусков в каждом столбце.  
- Удалили столбцы, где более 80 % значений отсутствовали (`weight`, `max_glu_serum`, `A1Cresult`), а также служебные идентификаторы (`encounter_id`, `patient_nbr`) — они не помогают модели.  
- Оставшиеся пропуски заполнили меткой `"Unknown"`.  
- Преобразовали текстовый столбец `age` (например, `"[30-40)"`) в числовой средний возраст.  
- Сохранили чистый датафрейм в `data/processed/df_clean.csv`.

**Зачем**  
Чтобы все признаки имели корректный формат (числа вместо строк), а пропуски никуда не «утекали», и модель могла учиться без ошибок.

---

## 2. Исследовательский анализ (`2_exploratory_analysis.ipynb`)

**Что** рассмотрели  
- Гистограммы и box-plot для числовых фич (время в стационаре, количество процедур, лекарств и т.д.).  
- Столбчатые диаграммы для категорий (`race`, `gender`, тип поступления…).  
- Матрицу корреляций по числам — чтобы найти сильные связи и понять, что можно оставить, а что, возможно, объединить.  
- Распределение целевой переменной `readmitted`:
  - NO: ~54 %  
  - > 30 дней: ~35 %  
  - < 30 дней: ~11 %

**Выводы**  
- Большинство пациентов не возвращается (< 30 дней), класс повторных госпитализаций малозаметен (11 %) — есть дисбаланс.  
- Числовые признаки в целом без экстремальных выбросов, но некоторые требуют стандартизации.

---

## 3. Базовая модель классификации (`3_baseline_model.ipynb`)

**Что** построили  
- Бинаризовали таргет:   
  - 1 → повтор ≤ 30 дней  
  - 0 → иначе  
- Строили логистическую регрессию с дефолтными настройками.

**Результаты**  

ROC AUC: 0.637
Precision (1): 0.17
Recall (1): 0.48
F1 (1): 0.25
Accuracy: 0.68


**Почему так**  
Логрег проста в реализации, даёт базовое представление о сложности задачи и служит отправной точкой.

---

## 4. Продвинутые модели (`4_advanced_models.ipynb`)

**Что пробовали**  
- **RandomForest**: AUC ≈ 0.65, но почти 0 % recall для малого класса.  
- **XGBoost (дефолт)**: AUC ≈ 0.667, recall ~ 0.56, precision низкий.  
- **Нейросеть (Keras)**: AUC ≈ 0.675, но тоже слаб в предсказании повторов.

**Итог**  
Нейросеть и XGBoost чуть лучше базовой логрегрессии по AUC, однако точность для класса «< 30 дней» по-прежнему низка.

---

## 5. Настройка гиперпараметров (`5_hyperparameter_tuning.ipynb`)

**Как** настраивали  
- Случайный поиск по параметрам XGBoost:  
  - `n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`, `gamma`.  
- 30 итераций, 3-fold CV.

**Лучшие параметры**  

n_estimators: 300
max_depth: 3
learning_rate: 0.10
subsample: 0.80
colsample_bytree: 0.60
gamma: 1

**Итоговая AUC:** **0.69**

**Коротко о дальнейших улучшениях**  
- Поэкспериментировать с **balancing** (SMOTE, class_weight).  
- Настроить **калибровку вероятностей**.  
- Попробовать **энсамбли** (stacking разных моделей).

---

## 6. Регрессия длительности лечения (`6_regression_time_in_hospital.ipynb`)

**Что делали**  
1. **Baseline**: предсказание медианой → MAE = 1.722 дня, RMSE = 2.279.  
2. **LinearRegression** без регуляризации — то же.  
3. **Ridge**/Lasso:
   - Ridge (α=1) → MAE = 1.714, RMSE = 2.267  
   - Ridge (α tuned → 10) → MAE = 1.711, RMSE = 2.265  
   - Lasso (α tuned) → чуть хуже.

**Вывод**  
Ridge с α=10 показал лучший результат — средняя ошибка ≈ 1.71 дня.

---

## Общие рекомендации и дальнейшие шаги

- Классификация:
  - Балансировка классов (SMOTE/undersampling).  
  - Калибровка и подбор порога.  
  - Попробовать LightGBM или StackingClassifier.

- Регрессия:
  - ElasticNet или нелинейные регрессоры (RandomForestRegressor, XGBRegressor).  
  - Добавить полиномиальные взаимодействия признаков.

---

**Заключение!**  
Вся работа собрана в «7_Final_submission.ipynb» — запустив его, вы получите полный повтор всех этапов: код, выводы, графики и рекомендации.